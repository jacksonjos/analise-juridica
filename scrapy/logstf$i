2015-07-28 16:54:50-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:54:50-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:54:50-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:54:50-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:54:51-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:54:51-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:54:51-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:54:51-0300 [stf] INFO: Spider opened
2015-07-28 16:54:51-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:54:51-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:54:51-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 54, 51, 975763),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 54, 51, 743466)}
2015-07-28 16:54:51-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:54:52-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:54:52-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:54:52-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:54:52-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:54:53-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:54:53-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:54:53-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:54:53-0300 [stf] INFO: Spider opened
2015-07-28 16:54:53-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:54:54-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:54:54-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 54, 54, 187025),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 54, 53, 314164)}
2015-07-28 16:54:54-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:54:54-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:54:54-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:54:54-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:54:54-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:54:55-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:54:55-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:54:55-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:54:55-0300 [stf] INFO: Spider opened
2015-07-28 16:54:55-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:54:56-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:54:56-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 54, 56, 393746),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 54, 55, 522737)}
2015-07-28 16:54:56-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:54:56-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:54:56-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:54:56-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:54:56-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:54:57-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:54:57-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:54:57-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:54:57-0300 [stf] INFO: Spider opened
2015-07-28 16:54:57-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:54:58-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:54:58-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 54, 58, 704779),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 54, 57, 731701)}
2015-07-28 16:54:58-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:54:58-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:54:58-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:54:58-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:54:58-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:00-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:00-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:00-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:00-0300 [stf] INFO: Spider opened
2015-07-28 16:55:00-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:00-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:00-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 0, 897074),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 0, 48313)}
2015-07-28 16:55:00-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:01-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:01-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:01-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:01-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:02-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:02-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:02-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:02-0300 [stf] INFO: Spider opened
2015-07-28 16:55:02-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:03-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:03-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 3, 128583),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 2, 249031)}
2015-07-28 16:55:03-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:03-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:03-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:03-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:03-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:04-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:04-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:04-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:04-0300 [stf] INFO: Spider opened
2015-07-28 16:55:04-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:05-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:05-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 5, 367575),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 4, 485780)}
2015-07-28 16:55:05-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:05-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:05-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:05-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:05-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:06-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:06-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:06-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:06-0300 [stf] INFO: Spider opened
2015-07-28 16:55:06-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:07-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:07-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 7, 40848),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 6, 703797)}
2015-07-28 16:55:07-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:07-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:07-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:07-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:07-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:08-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:08-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:08-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:08-0300 [stf] INFO: Spider opened
2015-07-28 16:55:08-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:09-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:09-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 9, 595473),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 8, 389290)}
2015-07-28 16:55:09-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:09-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:09-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:09-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:09-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:10-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:10-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:10-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:10-0300 [stf] INFO: Spider opened
2015-07-28 16:55:10-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:11-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:11-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 11, 854427),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 10, 934489)}
2015-07-28 16:55:11-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:12-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:12-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:12-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:12-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:13-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:13-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:13-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:13-0300 [stf] INFO: Spider opened
2015-07-28 16:55:13-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:14-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:14-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 14, 140413),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 13, 204235)}
2015-07-28 16:55:14-0300 [stf] INFO: Spider closed (finished)
2015-07-28 16:55:14-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 16:55:14-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 16:55:14-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 16:55:14-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 16:55:15-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 16:55:15-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 16:55:15-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 16:55:15-0300 [stf] INFO: Spider opened
2015-07-28 16:55:15-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 16:55:15-0300 [stf] INFO: Closing spider (finished)
2015-07-28 16:55:15-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 19, 55, 15, 774644),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 19, 55, 15, 486616)}
2015-07-28 16:55:15-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:02:48-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:02:48-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:02:48-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:02:48-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:02:49-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:02:49-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:02:49-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:02:49-0300 [stf] INFO: Spider opened
2015-07-28 17:02:49-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:02:50-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:02:50-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 2, 50, 570559),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 2, 49, 608523)}
2015-07-28 17:02:50-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:02:50-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:02:50-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:02:50-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:02:50-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:02:51-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:02:51-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:02:51-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:02:51-0300 [stf] INFO: Spider opened
2015-07-28 17:02:51-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:02:52-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:02:52-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 2, 52, 157340),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 2, 51, 918306)}
2015-07-28 17:02:52-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:02:52-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:02:52-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:02:52-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:02:52-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:02:53-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:02:53-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:02:53-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:02:53-0300 [stf] INFO: Spider opened
2015-07-28 17:02:53-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:02:53-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:02:53-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1306,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 2, 53, 729932),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 2, 53, 497462)}
2015-07-28 17:02:53-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:02:53-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:02:53-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:02:53-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:02:53-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:02:55-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:02:55-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:02:55-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:02:55-0300 [stf] INFO: Spider opened
2015-07-28 17:02:55-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:02:55-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:02:55-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:02:55-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 344,
	 'downloader/request_count': 1,
	 'downloader/request_method_count/GET': 1,
	 'downloader/response_bytes': 1608,
	 'downloader/response_count': 1,
	 'downloader/response_status_count/500': 1,
	 'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 2, 55, 175442),
	 'log_count/INFO': 8,
	 'scheduler/dequeued': 1,
	 'scheduler/dequeued/memory': 1,
	 'scheduler/enqueued': 2,
	 'scheduler/enqueued/memory': 2,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 2, 55, 76943)}
2015-07-28 17:02:55-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:02:55-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:02:55-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:02:55-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:02:55-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:02:56-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:02:56-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:02:56-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:02:56-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:02:56-0300 [stf] INFO: Spider opened
2015-07-28 17:02:56-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:02:56-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:02:56-0300 [stf] INFO: Dumping Scrapy stats:
	{'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 2, 56, 545042),
	 'log_count/INFO': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 2, 56, 543134)}
2015-07-28 17:02:56-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:02:56-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:02:56-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:02:56-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:02:56-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:02:57-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:02:57-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:02:57-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:02:57-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:02:57-0300 [stf] INFO: Spider opened
2015-07-28 17:02:57-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:02:57-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:02:57-0300 [stf] INFO: Dumping Scrapy stats:
	{'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 2, 57, 708016),
	 'log_count/INFO': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 2, 57, 705064)}
2015-07-28 17:02:57-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:03:30-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:30-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:30-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:30-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:31-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:31-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:31-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:31-0300 [stf] INFO: Spider opened
2015-07-28 17:03:31-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:31-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:31-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 31, 607494),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 31, 366687)}
2015-07-28 17:03:31-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:31-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:31-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:31-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:31-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:32-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:32-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:32-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:32-0300 [stf] INFO: Spider opened
2015-07-28 17:03:32-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:33-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:33-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 33, 182906),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 32, 945606)}
2015-07-28 17:03:33-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:33-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:33-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:33-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:33-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:34-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:34-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:34-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:34-0300 [stf] INFO: Spider opened
2015-07-28 17:03:34-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:35-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:35-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 35, 438044),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 34, 522316)}
2015-07-28 17:03:35-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:35-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:35-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:35-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:35-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:36-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:36-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:36-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:36-0300 [stf] INFO: Spider opened
2015-07-28 17:03:36-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:37-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:37-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 37, 655049),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 36, 787817)}
2015-07-28 17:03:37-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:37-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:37-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:37-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:37-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:38-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:38-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:39-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:39-0300 [stf] INFO: Spider opened
2015-07-28 17:03:39-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:39-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:39-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 39, 235119),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 39, 3129)}
2015-07-28 17:03:39-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:39-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:39-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:39-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:39-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:40-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:40-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:40-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:40-0300 [stf] INFO: Spider opened
2015-07-28 17:03:40-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:41-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:41-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 41, 502427),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 40, 568526)}
2015-07-28 17:03:41-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:41-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:41-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:41-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:41-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:42-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:42-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:42-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:42-0300 [stf] INFO: Spider opened
2015-07-28 17:03:42-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:43-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:43-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 43, 825907),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 42, 855149)}
2015-07-28 17:03:43-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:44-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:44-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:44-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:44-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:45-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:45-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:45-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:45-0300 [stf] INFO: Spider opened
2015-07-28 17:03:45-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:45-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:45-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 45, 472607),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 45, 172482)}
2015-07-28 17:03:45-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:45-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:45-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:45-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:45-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:46-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:46-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:46-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:46-0300 [stf] INFO: Spider opened
2015-07-28 17:03:46-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:47-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:47-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 47, 40362),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 46, 809554)}
2015-07-28 17:03:47-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:47-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:47-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:47-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:47-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:48-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:48-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:48-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:48-0300 [stf] INFO: Spider opened
2015-07-28 17:03:48-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:48-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:48-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 48, 656800),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 48, 394291)}
2015-07-28 17:03:48-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:48-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:48-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:48-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:48-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:49-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:49-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:49-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:49-0300 [stf] INFO: Spider opened
2015-07-28 17:03:50-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:50-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:03:50-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 50, 876650),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 50, 364)}
2015-07-28 17:03:50-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:03:51-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:03:51-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:03:51-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:03:51-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:03:51-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:03:51-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:03:51-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:03:51-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:03:51-0300 [stf] INFO: Spider opened
2015-07-28 17:03:51-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:03:51-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:03:51-0300 [stf] INFO: Dumping Scrapy stats:
	{'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 3, 51, 974009),
	 'log_count/INFO': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 3, 51, 972143)}
2015-07-28 17:03:51-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:04:29-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:04:29-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:04:29-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:04:29-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:04:30-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:04:30-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:04:30-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:04:30-0300 [stf] INFO: Spider opened
2015-07-28 17:04:30-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:04:30-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:04:30-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 4, 30, 972799),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 4, 30, 704584)}
2015-07-28 17:04:30-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:04:31-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:04:31-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:04:31-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:04:31-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:04:32-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:04:32-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:04:32-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:04:32-0300 [stf] INFO: Spider opened
2015-07-28 17:04:32-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:04:32-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:04:32-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 4, 32, 571818),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 4, 32, 313303)}
2015-07-28 17:04:32-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:04:56-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:04:56-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:04:56-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:04:56-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:04:56-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:04:56-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:04:56-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:04:56-0300 [stf] INFO: Spider opened
2015-07-28 17:04:56-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:04:57-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:04:57-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 4, 57, 251145),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 4, 56, 996332)}
2015-07-28 17:04:57-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:04:57-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:04:57-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:04:57-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:04:57-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:04:58-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:04:58-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:04:58-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:04:58-0300 [stf] INFO: Spider opened
2015-07-28 17:04:58-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:04:59-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:04:59-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 4, 59, 483805),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 4, 58, 599144)}
2015-07-28 17:04:59-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:06:41-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:06:41-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:06:41-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:06:41-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:06:42-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:06:42-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:06:42-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:06:42-0300 [stf] INFO: Spider opened
2015-07-28 17:06:42-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:06:42-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:06:42-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 6, 42, 750648),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 6, 42, 536604)}
2015-07-28 17:06:42-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:06:42-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:06:42-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:06:42-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:06:43-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:06:44-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:06:44-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:06:44-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:06:44-0300 [stf] INFO: Spider opened
2015-07-28 17:06:44-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:06:44-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:06:44-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:06:44-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 6, 44, 976795),
	 'log_count/INFO': 8,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 6, 44, 101797)}
2015-07-28 17:06:44-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:08:47-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:08:47-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:08:47-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:08:47-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:08:48-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:08:48-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:08:48-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:08:48-0300 [stf] INFO: Spider opened
2015-07-28 17:08:48-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:08:49-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:08:49-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 8, 49, 124043),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 8, 48, 868792)}
2015-07-28 17:08:49-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:08:49-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:08:49-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:08:49-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:08:49-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:08:50-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:08:50-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:08:50-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:08:50-0300 [stf] INFO: Spider opened
2015-07-28 17:08:50-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:08:51-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:08:51-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1288,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 8, 51, 318763),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 8, 50, 481042)}
2015-07-28 17:08:51-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:09:09-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:09:09-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:09:09-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:09:09-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:09:10-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:09:10-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:09:10-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:09:10-0300 [stf] INFO: Spider opened
2015-07-28 17:09:10-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:09:11-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:09:11-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1300,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 9, 11, 318836),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 9, 10, 447559)}
2015-07-28 17:09:11-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:09:11-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:09:11-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:09:11-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:09:11-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:09:12-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:09:12-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:09:12-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:09:12-0300 [stf] INFO: Spider opened
2015-07-28 17:09:12-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:09:13-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:09:13-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1300,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 9, 13, 586004),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 9, 12, 660464)}
2015-07-28 17:09:13-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:10:08-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:10:08-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:10:08-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:10:08-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:10:09-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:10:09-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:10:09-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:10:09-0300 [stf] INFO: Spider opened
2015-07-28 17:10:09-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:10:10-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:10:10-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1300,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 10, 10, 637585),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 10, 9, 773267)}
2015-07-28 17:10:10-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:10:10-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:10:10-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:10:10-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:10:10-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:10:10-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:10:11-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:10:11-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:10:11-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:10:11-0300 [stf] INFO: Spider opened
2015-07-28 17:10:11-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:10:11-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:10:11-0300 [stf] INFO: Dumping Scrapy stats:
	{'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 10, 11, 45537),
	 'log_count/INFO': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 10, 11, 43594)}
2015-07-28 17:10:11-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:10:24-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:10:24-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:10:24-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:10:24-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:10:25-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:10:25-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:10:25-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:10:25-0300 [stf] INFO: Spider opened
2015-07-28 17:10:25-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:10:26-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:10:26-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1300,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 10, 26, 837352),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 10, 25, 860567)}
2015-07-28 17:10:26-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:10:27-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:10:27-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:10:27-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:10:27-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:10:28-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:10:28-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:10:28-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:10:28-0300 [stf] INFO: Spider opened
2015-07-28 17:10:28-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:10:29-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:10:29-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1300,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 10, 29, 144264),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 10, 28, 182240)}
2015-07-28 17:10:29-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:11:19-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:11:19-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:11:19-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:11:19-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:11:20-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:11:20-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:11:20-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:11:20-0300 [stf] INFO: Spider opened
2015-07-28 17:11:20-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:11:21-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:11:21-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 1300,
	 'downloader/request_count': 3,
	 'downloader/request_method_count/GET': 3,
	 'downloader/response_bytes': 4452,
	 'downloader/response_count': 3,
	 'downloader/response_status_count/500': 3,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 11, 21, 253133),
	 'log_count/INFO': 7,
	 'response_received_count': 1,
	 'scheduler/dequeued': 3,
	 'scheduler/dequeued/memory': 3,
	 'scheduler/enqueued': 3,
	 'scheduler/enqueued/memory': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 11, 20, 405836)}
2015-07-28 17:11:21-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:11:21-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:11:21-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:11:21-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:11:21-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:11:22-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:11:22-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:11:22-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:11:22-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:11:22-0300 [stf] INFO: Spider opened
2015-07-28 17:11:22-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:11:22-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:11:22-0300 [stf] INFO: Dumping Scrapy stats:
	{'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 11, 22, 528553),
	 'log_count/INFO': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 11, 22, 526666)}
2015-07-28 17:11:22-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:14:51-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:14:51-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:14:51-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:14:51-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:14:52-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:14:52-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:14:52-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:14:52-0300 [stf] INFO: Spider opened
2015-07-28 17:14:52-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:15:04-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:15:04-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:15:04-0300 [scrapy] INFO: Received SIGINT twice, forcing unclean shutdown
2015-07-28 17:15:04-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:15:04-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:15:04-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:15:04-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:15:05-0300 [scrapy] INFO: Received SIGINT, shutting down gracefully. Send again to force 
2015-07-28 17:15:05-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:15:05-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:15:05-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:15:05-0300 [stf] INFO: Spider opened
2015-07-28 17:15:05-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:15:05-0300 [stf] INFO: Closing spider (shutdown)
2015-07-28 17:15:05-0300 [stf] INFO: Dumping Scrapy stats:
	{'finish_reason': 'shutdown',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 15, 5, 591085),
	 'log_count/INFO': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 15, 5, 589169)}
2015-07-28 17:15:05-0300 [stf] INFO: Spider closed (shutdown)
2015-07-28 17:16:52-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:16:52-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:16:52-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:16:53-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:16:54-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:16:54-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:16:54-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:16:54-0300 [stf] INFO: Spider opened
2015-07-28 17:16:54-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:17:54-0300 [stf] INFO: Crawled 32 pages (at 32 pages/min), scraped 308 items (at 308 items/min)
2015-07-28 17:18:54-0300 [stf] INFO: Crawled 67 pages (at 35 pages/min), scraped 658 items (at 350 items/min)
2015-07-28 17:19:54-0300 [stf] INFO: Crawled 97 pages (at 30 pages/min), scraped 958 items (at 300 items/min)
2015-07-28 17:20:54-0300 [stf] INFO: Crawled 133 pages (at 36 pages/min), scraped 1318 items (at 360 items/min)
2015-07-28 17:21:54-0300 [stf] INFO: Crawled 167 pages (at 34 pages/min), scraped 1658 items (at 340 items/min)
2015-07-28 17:22:54-0300 [stf] INFO: Crawled 200 pages (at 33 pages/min), scraped 1988 items (at 330 items/min)
2015-07-28 17:22:54-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20010101%29%28%40JULG+%3C%3D+20020101%29&pagina=116&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 17:23:54-0300 [stf] INFO: Crawled 231 pages (at 31 pages/min), scraped 2297 items (at 309 items/min)
2015-07-28 17:24:54-0300 [stf] INFO: Crawled 263 pages (at 32 pages/min), scraped 2617 items (at 320 items/min)
2015-07-28 17:25:54-0300 [stf] INFO: Crawled 295 pages (at 32 pages/min), scraped 2937 items (at 320 items/min)
2015-07-28 17:26:01-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:26:01-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 197395,
	 'downloader/request_count': 302,
	 'downloader/request_method_count/GET': 302,
	 'downloader/response_bytes': 23838990,
	 'downloader/response_count': 302,
	 'downloader/response_status_count/200': 301,
	 'downloader/response_status_count/500': 1,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 26, 1, 492904),
	 'item_scraped_count': 2997,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 16,
	 'request_depth_max': 1,
	 'response_received_count': 301,
	 'scheduler/dequeued': 302,
	 'scheduler/dequeued/memory': 302,
	 'scheduler/enqueued': 302,
	 'scheduler/enqueued/memory': 302,
	 'spider_exceptions/IOError': 1,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 16, 54, 89471)}
2015-07-28 17:26:01-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:26:01-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:26:01-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:26:01-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:26:01-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:26:02-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:26:02-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:26:02-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:26:02-0300 [stf] INFO: Spider opened
2015-07-28 17:26:02-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:27:02-0300 [stf] INFO: Crawled 140 pages (at 140 pages/min), scraped 1383 items (at 1383 items/min)
2015-07-28 17:27:59-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20020101%29%28%40JULG+%3C%3D+20030101%29&pagina=125&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 26-27: ordinal not in range(128)
	
2015-07-28 17:28:02-0300 [stf] INFO: Crawled 284 pages (at 144 pages/min), scraped 2814 items (at 1431 items/min)
2015-07-28 17:28:13-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20020101%29%28%40JULG+%3C%3D+20030101%29&pagina=92&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 26-27: ordinal not in range(128)
	
2015-07-28 17:28:24-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20020101%29%28%40JULG+%3C%3D+20030101%29&pagina=60&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 0: ordinal not in range(128)
	
2015-07-28 17:28:45-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:28:45-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 251105,
	 'downloader/request_count': 384,
	 'downloader/request_method_count/GET': 384,
	 'downloader/response_bytes': 30435960,
	 'downloader/response_count': 384,
	 'downloader/response_status_count/200': 384,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 28, 45, 153852),
	 'item_scraped_count': 3803,
	 'log_count/ERROR': 3,
	 'log_count/INFO': 9,
	 'request_depth_max': 1,
	 'response_received_count': 384,
	 'scheduler/dequeued': 384,
	 'scheduler/dequeued/memory': 384,
	 'scheduler/enqueued': 384,
	 'scheduler/enqueued/memory': 384,
	 'spider_exceptions/UnicodeEncodeError': 3,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 26, 2, 844170)}
2015-07-28 17:28:45-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:28:45-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:28:45-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:28:45-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:28:45-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:28:46-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:28:46-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:28:46-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:28:46-0300 [stf] INFO: Spider opened
2015-07-28 17:28:46-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:28:50-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20030101%29%28%40JULG+%3C%3D+20040101%29&pagina=3&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc9' in position 26: ordinal not in range(128)
	
2015-07-28 17:29:46-0300 [stf] INFO: Crawled 37 pages (at 37 pages/min), scraped 353 items (at 353 items/min)
2015-07-28 17:30:01-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20030101%29%28%40JULG+%3C%3D+20040101%29&pagina=206&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 14: ordinal not in range(128)
	
2015-07-28 17:30:46-0300 [stf] INFO: Crawled 72 pages (at 35 pages/min), scraped 697 items (at 344 items/min)
2015-07-28 17:31:13-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20030101%29%28%40JULG+%3C%3D+20040101%29&pagina=165&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc1' in position 27: ordinal not in range(128)
	
2015-07-28 17:31:18-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20030101%29%28%40JULG+%3C%3D+20040101%29&pagina=163&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 14-15: ordinal not in range(128)
	
2015-07-28 17:31:46-0300 [stf] INFO: Crawled 106 pages (at 34 pages/min), scraped 1024 items (at 327 items/min)
2015-07-28 17:32:46-0300 [stf] INFO: Crawled 135 pages (at 29 pages/min), scraped 1314 items (at 290 items/min)
2015-07-28 17:33:46-0300 [stf] INFO: Crawled 163 pages (at 28 pages/min), scraped 1594 items (at 280 items/min)
2015-07-28 17:34:46-0300 [stf] INFO: Crawled 195 pages (at 32 pages/min), scraped 1914 items (at 320 items/min)
2015-07-28 17:34:54-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20030101%29%28%40JULG+%3C%3D+20040101%29&pagina=53&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:35:46-0300 [stf] INFO: Crawled 225 pages (at 30 pages/min), scraped 2213 items (at 299 items/min)
2015-07-28 17:36:04-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:36:04-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 153510,
	 'downloader/request_count': 235,
	 'downloader/request_method_count/GET': 235,
	 'downloader/response_bytes': 18952917,
	 'downloader/response_count': 235,
	 'downloader/response_status_count/200': 235,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 36, 4, 177461),
	 'item_scraped_count': 2313,
	 'log_count/ERROR': 5,
	 'log_count/INFO': 14,
	 'request_depth_max': 1,
	 'response_received_count': 235,
	 'scheduler/dequeued': 235,
	 'scheduler/dequeued/memory': 235,
	 'scheduler/enqueued': 235,
	 'scheduler/enqueued/memory': 235,
	 'spider_exceptions/IndexError': 1,
	 'spider_exceptions/UnicodeEncodeError': 4,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 28, 46, 485810)}
2015-07-28 17:36:04-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:36:04-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:36:04-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:36:04-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:36:04-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:36:05-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:36:05-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:36:05-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:36:05-0300 [stf] INFO: Spider opened
2015-07-28 17:36:05-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:37:02-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=245&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:37:05-0300 [stf] INFO: Crawled 138 pages (at 138 pages/min), scraped 1358 items (at 1358 items/min)
2015-07-28 17:37:10-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=228&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:37:15-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=214&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:37:20-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=202&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:38:01-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=108&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:38:05-0300 [stf] INFO: Crawled 277 pages (at 139 pages/min), scraped 2723 items (at 1365 items/min)
2015-07-28 17:38:40-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=13&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:38:42-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:38:42-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 234730,
	 'downloader/request_count': 359,
	 'downloader/request_method_count/GET': 359,
	 'downloader/response_bytes': 28580908,
	 'downloader/response_count': 359,
	 'downloader/response_status_count/200': 359,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 38, 42, 526576),
	 'item_scraped_count': 3537,
	 'log_count/ERROR': 6,
	 'log_count/INFO': 9,
	 'request_depth_max': 1,
	 'response_received_count': 359,
	 'scheduler/dequeued': 359,
	 'scheduler/dequeued/memory': 359,
	 'scheduler/enqueued': 359,
	 'scheduler/enqueued/memory': 359,
	 'spider_exceptions/IndexError': 6,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 36, 5, 536535)}
2015-07-28 17:38:42-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:38:42-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:38:42-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:38:42-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:38:42-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:38:43-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:38:43-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:38:43-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:38:43-0300 [stf] INFO: Spider opened
2015-07-28 17:38:43-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:39:43-0300 [stf] INFO: Crawled 39 pages (at 39 pages/min), scraped 378 items (at 378 items/min)
2015-07-28 17:40:12-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=413&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc9' in position 27: ordinal not in range(128)
	
2015-07-28 17:40:43-0300 [stf] INFO: Crawled 78 pages (at 39 pages/min), scraped 762 items (at 384 items/min)
2015-07-28 17:41:43-0300 [stf] INFO: Crawled 119 pages (at 41 pages/min), scraped 1172 items (at 410 items/min)
2015-07-28 17:42:23-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=318&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:42:43-0300 [stf] INFO: Crawled 169 pages (at 50 pages/min), scraped 1670 items (at 498 items/min)
2015-07-28 17:43:43-0300 [stf] INFO: Crawled 211 pages (at 42 pages/min), scraped 2090 items (at 420 items/min)
2015-07-28 17:44:24-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=233&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc1' in position 29: ordinal not in range(128)
	
2015-07-28 17:44:43-0300 [stf] INFO: Crawled 248 pages (at 37 pages/min), scraped 2452 items (at 362 items/min)
2015-07-28 17:45:35-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=186&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:45:43-0300 [stf] INFO: Crawled 288 pages (at 40 pages/min), scraped 2848 items (at 396 items/min)
2015-07-28 17:45:55-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=171&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:46:43-0300 [stf] INFO: Crawled 324 pages (at 36 pages/min), scraped 3204 items (at 356 items/min)
2015-07-28 17:47:43-0300 [stf] INFO: Crawled 362 pages (at 38 pages/min), scraped 3584 items (at 380 items/min)
2015-07-28 17:48:43-0300 [stf] INFO: Crawled 398 pages (at 36 pages/min), scraped 3944 items (at 360 items/min)
2015-07-28 17:49:43-0300 [stf] INFO: Crawled 432 pages (at 34 pages/min), scraped 4284 items (at 340 items/min)
2015-07-28 17:50:19-0300 [stf] INFO: Closing spider (finished)
2015-07-28 17:50:19-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 295645,
	 'downloader/request_count': 452,
	 'downloader/request_method_count/GET': 452,
	 'downloader/response_bytes': 36569118,
	 'downloader/response_count': 452,
	 'downloader/response_status_count/200': 452,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 20, 50, 19, 238069),
	 'item_scraped_count': 4484,
	 'log_count/ERROR': 5,
	 'log_count/INFO': 18,
	 'request_depth_max': 1,
	 'response_received_count': 452,
	 'scheduler/dequeued': 452,
	 'scheduler/dequeued/memory': 452,
	 'scheduler/enqueued': 452,
	 'scheduler/enqueued/memory': 452,
	 'spider_exceptions/IndexError': 3,
	 'spider_exceptions/UnicodeEncodeError': 2,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 38, 43, 879454)}
2015-07-28 17:50:19-0300 [stf] INFO: Spider closed (finished)
2015-07-28 17:50:19-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 17:50:19-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 17:50:19-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 17:50:19-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 17:50:20-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 17:50:20-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 17:50:20-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 17:50:20-0300 [stf] INFO: Spider opened
2015-07-28 17:50:20-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 17:51:20-0300 [stf] INFO: Crawled 38 pages (at 38 pages/min), scraped 361 items (at 361 items/min)
2015-07-28 17:52:20-0300 [stf] INFO: Crawled 74 pages (at 36 pages/min), scraped 721 items (at 360 items/min)
2015-07-28 17:52:26-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=342&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:53:20-0300 [stf] INFO: Crawled 106 pages (at 32 pages/min), scraped 1031 items (at 310 items/min)
2015-07-28 17:54:20-0300 [stf] INFO: Crawled 141 pages (at 35 pages/min), scraped 1381 items (at 350 items/min)
2015-07-28 17:55:00-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=251&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:55:20-0300 [stf] INFO: Crawled 180 pages (at 39 pages/min), scraped 1767 items (at 386 items/min)
2015-07-28 17:56:20-0300 [stf] INFO: Crawled 220 pages (at 40 pages/min), scraped 2167 items (at 400 items/min)
2015-07-28 17:56:52-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=178&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:57:20-0300 [stf] INFO: Crawled 258 pages (at 38 pages/min), scraped 2539 items (at 372 items/min)
2015-07-28 17:58:20-0300 [stf] INFO: Crawled 297 pages (at 39 pages/min), scraped 2929 items (at 390 items/min)
2015-07-28 17:58:46-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=104&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:59:09-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=89&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 17:59:20-0300 [stf] INFO: Crawled 337 pages (at 40 pages/min), scraped 3311 items (at 382 items/min)
2015-07-28 18:00:20-0300 [stf] INFO: Crawled 373 pages (at 36 pages/min), scraped 3671 items (at 360 items/min)
2015-07-28 18:01:09-0300 [stf] INFO: Closing spider (finished)
2015-07-28 18:01:09-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 262895,
	 'downloader/request_count': 402,
	 'downloader/request_method_count/GET': 402,
	 'downloader/response_bytes': 32355154,
	 'downloader/response_count': 402,
	 'downloader/response_status_count/200': 402,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 21, 1, 9, 386474),
	 'item_scraped_count': 3961,
	 'log_count/ERROR': 5,
	 'log_count/INFO': 17,
	 'request_depth_max': 1,
	 'response_received_count': 402,
	 'scheduler/dequeued': 402,
	 'scheduler/dequeued/memory': 402,
	 'scheduler/enqueued': 402,
	 'scheduler/enqueued/memory': 402,
	 'spider_exceptions/IndexError': 5,
	 'start_time': datetime.datetime(2015, 7, 28, 20, 50, 20, 603255)}
2015-07-28 18:01:09-0300 [stf] INFO: Spider closed (finished)
2015-07-28 18:01:09-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 18:01:09-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 18:01:09-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 18:01:09-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 18:01:10-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 18:01:10-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 18:01:10-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 18:01:10-0300 [stf] INFO: Spider opened
2015-07-28 18:01:10-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 18:02:10-0300 [stf] INFO: Crawled 45 pages (at 45 pages/min), scraped 438 items (at 438 items/min)
2015-07-28 18:02:27-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=420&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:03:10-0300 [stf] INFO: Crawled 85 pages (at 40 pages/min), scraped 832 items (at 394 items/min)
2015-07-28 18:04:10-0300 [stf] INFO: Crawled 129 pages (at 44 pages/min), scraped 1272 items (at 440 items/min)
2015-07-28 18:05:10-0300 [stf] INFO: Crawled 171 pages (at 42 pages/min), scraped 1692 items (at 420 items/min)
2015-07-28 18:06:10-0300 [stf] INFO: Crawled 211 pages (at 40 pages/min), scraped 2092 items (at 400 items/min)
2015-07-28 18:07:10-0300 [stf] INFO: Crawled 245 pages (at 34 pages/min), scraped 2432 items (at 340 items/min)
2015-07-28 18:07:55-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=201&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc1' in position 5: ordinal not in range(128)
	
2015-07-28 18:08:10-0300 [stf] INFO: Crawled 285 pages (at 40 pages/min), scraped 2830 items (at 398 items/min)
2015-07-28 18:08:19-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=185&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:09:10-0300 [stf] INFO: Crawled 326 pages (at 41 pages/min), scraped 3236 items (at 406 items/min)
2015-07-28 18:10:10-0300 [stf] INFO: Crawled 363 pages (at 37 pages/min), scraped 3606 items (at 370 items/min)
2015-07-28 18:10:33-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=102&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:11:10-0300 [stf] INFO: Crawled 403 pages (at 40 pages/min), scraped 4002 items (at 396 items/min)
2015-07-28 18:12:10-0300 [stf] INFO: Crawled 446 pages (at 43 pages/min), scraped 4432 items (at 430 items/min)
2015-07-28 18:12:29-0300 [stf] INFO: Closing spider (finished)
2015-07-28 18:12:29-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 300885,
	 'downloader/request_count': 460,
	 'downloader/request_method_count/GET': 460,
	 'downloader/response_bytes': 39778049,
	 'downloader/response_count': 460,
	 'downloader/response_status_count/200': 460,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 21, 12, 29, 202538),
	 'item_scraped_count': 4572,
	 'log_count/ERROR': 4,
	 'log_count/INFO': 18,
	 'request_depth_max': 1,
	 'response_received_count': 460,
	 'scheduler/dequeued': 460,
	 'scheduler/dequeued/memory': 460,
	 'scheduler/enqueued': 460,
	 'scheduler/enqueued/memory': 460,
	 'spider_exceptions/IndexError': 3,
	 'spider_exceptions/UnicodeEncodeError': 1,
	 'start_time': datetime.datetime(2015, 7, 28, 21, 1, 10, 732655)}
2015-07-28 18:12:29-0300 [stf] INFO: Spider closed (finished)
2015-07-28 18:12:29-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 18:12:29-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 18:12:29-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 18:12:29-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 18:12:30-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 18:12:30-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 18:12:30-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 18:12:30-0300 [stf] INFO: Spider opened
2015-07-28 18:12:30-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 18:13:30-0300 [stf] INFO: Crawled 33 pages (at 33 pages/min), scraped 317 items (at 317 items/min)
2015-07-28 18:14:30-0300 [stf] INFO: Crawled 67 pages (at 34 pages/min), scraped 657 items (at 340 items/min)
2015-07-28 18:15:30-0300 [stf] INFO: Crawled 104 pages (at 37 pages/min), scraped 1027 items (at 370 items/min)
2015-07-28 18:16:30-0300 [stf] INFO: Crawled 145 pages (at 41 pages/min), scraped 1437 items (at 410 items/min)
2015-07-28 18:17:30-0300 [stf] INFO: Crawled 186 pages (at 41 pages/min), scraped 1847 items (at 410 items/min)
2015-07-28 18:17:47-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=316&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:18:30-0300 [stf] INFO: Crawled 231 pages (at 45 pages/min), scraped 2291 items (at 444 items/min)
2015-07-28 18:19:29-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=240&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:19:30-0300 [stf] INFO: Crawled 275 pages (at 44 pages/min), scraped 2725 items (at 434 items/min)
2015-07-28 18:20:30-0300 [stf] INFO: Crawled 318 pages (at 43 pages/min), scraped 3155 items (at 430 items/min)
2015-07-28 18:21:30-0300 [stf] INFO: Crawled 358 pages (at 40 pages/min), scraped 3555 items (at 400 items/min)
2015-07-28 18:22:30-0300 [stf] INFO: Crawled 394 pages (at 36 pages/min), scraped 3915 items (at 360 items/min)
2015-07-28 18:22:39-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=113&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:23:30-0300 [stf] INFO: Crawled 437 pages (at 43 pages/min), scraped 4344 items (at 429 items/min)
2015-07-28 18:24:30-0300 [stf] INFO: Crawled 479 pages (at 42 pages/min), scraped 4764 items (at 420 items/min)
2015-07-28 18:24:55-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=2&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:24:56-0300 [stf] INFO: Closing spider (finished)
2015-07-28 18:24:56-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 325775,
	 'downloader/request_count': 498,
	 'downloader/request_method_count/GET': 498,
	 'downloader/response_bytes': 41297498,
	 'downloader/response_count': 498,
	 'downloader/response_status_count/200': 498,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 21, 24, 56, 670075),
	 'item_scraped_count': 4947,
	 'log_count/ERROR': 4,
	 'log_count/INFO': 19,
	 'request_depth_max': 1,
	 'response_received_count': 498,
	 'scheduler/dequeued': 498,
	 'scheduler/dequeued/memory': 498,
	 'scheduler/enqueued': 498,
	 'scheduler/enqueued/memory': 498,
	 'spider_exceptions/IndexError': 4,
	 'start_time': datetime.datetime(2015, 7, 28, 21, 12, 30, 548015)}
2015-07-28 18:24:56-0300 [stf] INFO: Spider closed (finished)
2015-07-28 18:24:56-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 18:24:56-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 18:24:56-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 18:24:56-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 18:24:57-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 18:24:57-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 18:24:58-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 18:24:58-0300 [stf] INFO: Spider opened
2015-07-28 18:24:58-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 18:25:58-0300 [stf] INFO: Crawled 33 pages (at 33 pages/min), scraped 320 items (at 320 items/min)
2015-07-28 18:26:58-0300 [stf] INFO: Crawled 74 pages (at 41 pages/min), scraped 730 items (at 410 items/min)
2015-07-28 18:27:58-0300 [stf] INFO: Crawled 115 pages (at 41 pages/min), scraped 1140 items (at 410 items/min)
2015-07-28 18:28:21-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=341&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xcd' in position 9: ordinal not in range(128)
	
2015-07-28 18:28:58-0300 [stf] INFO: Crawled 148 pages (at 33 pages/min), scraped 1468 items (at 328 items/min)
2015-07-28 18:29:58-0300 [stf] INFO: Crawled 184 pages (at 36 pages/min), scraped 1828 items (at 360 items/min)
2015-07-28 18:30:58-0300 [stf] INFO: Crawled 223 pages (at 39 pages/min), scraped 2218 items (at 390 items/min)
2015-07-28 18:31:58-0300 [stf] INFO: Crawled 255 pages (at 32 pages/min), scraped 2538 items (at 320 items/min)
2015-07-28 18:32:37-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=191&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:32:47-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=185&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:32:58-0300 [stf] INFO: Crawled 292 pages (at 37 pages/min), scraped 2898 items (at 360 items/min)
2015-07-28 18:33:41-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=156&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 18:33:45-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=153&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 11: ordinal not in range(128)
	
2015-07-28 18:33:46-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=152&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 17: ordinal not in range(128)
	
2015-07-28 18:33:58-0300 [stf] INFO: Crawled 325 pages (at 33 pages/min), scraped 3208 items (at 310 items/min)
2015-07-28 18:34:58-0300 [stf] INFO: Crawled 357 pages (at 32 pages/min), scraped 3528 items (at 320 items/min)
2015-07-28 18:35:09-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=104&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 8: ordinal not in range(128)
	
2015-07-28 18:35:19-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=96&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 29: ordinal not in range(128)
	
2015-07-28 18:35:23-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=94&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 29: ordinal not in range(128)
	
2015-07-28 18:35:31-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=88&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 29: ordinal not in range(128)
	
2015-07-28 18:35:41-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=82&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xba' in position 1: ordinal not in range(128)
	
2015-07-28 18:35:46-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=78&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 29: ordinal not in range(128)
	
2015-07-28 18:35:53-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=74&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 29: ordinal not in range(128)
	
2015-07-28 18:35:58-0300 [stf] INFO: Crawled 398 pages (at 41 pages/min), scraped 3900 items (at 372 items/min)
2015-07-28 18:35:59-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=70&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode characters in position 27-28: ordinal not in range(128)
	
2015-07-28 18:36:25-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=56&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc1' in position 7: ordinal not in range(128)
	
2015-07-28 18:36:26-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=55&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xcd' in position 2: ordinal not in range(128)
	
2015-07-28 18:36:27-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=54&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xba' in position 1: ordinal not in range(128)
	
2015-07-28 18:36:48-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=38&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc9' in position 3: ordinal not in range(128)
	
2015-07-28 18:36:58-0300 [stf] INFO: Crawled 439 pages (at 41 pages/min), scraped 4283 items (at 383 items/min)
2015-07-28 18:37:19-0300 [stf] INFO: Closing spider (finished)
2015-07-28 18:37:19-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 296300,
	 'downloader/request_count': 453,
	 'downloader/request_method_count/GET': 453,
	 'downloader/response_bytes': 36323733,
	 'downloader/response_count': 453,
	 'downloader/response_status_count/200': 453,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 21, 37, 19, 97623),
	 'item_scraped_count': 4423,
	 'log_count/ERROR': 18,
	 'log_count/INFO': 19,
	 'request_depth_max': 1,
	 'response_received_count': 453,
	 'scheduler/dequeued': 453,
	 'scheduler/dequeued/memory': 453,
	 'scheduler/enqueued': 453,
	 'scheduler/enqueued/memory': 453,
	 'spider_exceptions/IndexError': 3,
	 'spider_exceptions/UnicodeEncodeError': 15,
	 'start_time': datetime.datetime(2015, 7, 28, 21, 24, 58, 24681)}
2015-07-28 18:37:19-0300 [stf] INFO: Spider closed (finished)
2015-07-28 18:37:19-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 18:37:19-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 18:37:19-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 18:37:19-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 18:37:20-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 18:37:20-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 18:37:20-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 18:37:20-0300 [stf] INFO: Spider opened
2015-07-28 18:37:20-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 18:38:20-0300 [stf] INFO: Crawled 36 pages (at 36 pages/min), scraped 344 items (at 344 items/min)
2015-07-28 18:39:20-0300 [stf] INFO: Crawled 78 pages (at 42 pages/min), scraped 764 items (at 420 items/min)
2015-07-28 18:40:20-0300 [stf] INFO: Crawled 114 pages (at 36 pages/min), scraped 1124 items (at 360 items/min)
2015-07-28 18:41:20-0300 [stf] INFO: Crawled 156 pages (at 42 pages/min), scraped 1544 items (at 420 items/min)
2015-07-28 18:41:47-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=325&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 18:42:16-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=302&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 18:42:18-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=301&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xba' in position 30: ordinal not in range(128)
	
2015-07-28 18:42:20-0300 [stf] INFO: Crawled 203 pages (at 47 pages/min), scraped 1998 items (at 454 items/min)
2015-07-28 18:43:20-0300 [stf] INFO: Crawled 243 pages (at 40 pages/min), scraped 2398 items (at 400 items/min)
2015-07-28 18:43:33-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=252&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 18: ordinal not in range(128)
	
2015-07-28 18:44:20-0300 [stf] INFO: Crawled 278 pages (at 35 pages/min), scraped 2743 items (at 345 items/min)
2015-07-28 18:45:20-0300 [stf] INFO: Crawled 317 pages (at 39 pages/min), scraped 3133 items (at 390 items/min)
2015-07-28 18:45:26-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=180&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 18: ordinal not in range(128)
	
2015-07-28 18:45:49-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=167&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 18: ordinal not in range(128)
	
2015-07-28 18:46:20-0300 [stf] INFO: Crawled 361 pages (at 44 pages/min), scraped 3561 items (at 428 items/min)
2015-07-28 18:47:20-0300 [stf] INFO: Crawled 399 pages (at 38 pages/min), scraped 3941 items (at 380 items/min)
2015-07-28 18:47:32-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=94&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xda' in position 22: ordinal not in range(128)
	
2015-07-28 18:47:53-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=84&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 18:48:20-0300 [stf] INFO: Crawled 436 pages (at 37 pages/min), scraped 4295 items (at 354 items/min)
2015-07-28 18:49:20-0300 [stf] INFO: Crawled 478 pages (at 42 pages/min), scraped 4715 items (at 420 items/min)
2015-07-28 18:49:31-0300 [stf] INFO: Closing spider (finished)
2015-07-28 18:49:31-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 317260,
	 'downloader/request_count': 485,
	 'downloader/request_method_count/GET': 485,
	 'downloader/response_bytes': 38911008,
	 'downloader/response_count': 485,
	 'downloader/response_status_count/200': 485,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 21, 49, 31, 552840),
	 'item_scraped_count': 4785,
	 'log_count/ERROR': 8,
	 'log_count/INFO': 19,
	 'request_depth_max': 1,
	 'response_received_count': 485,
	 'scheduler/dequeued': 485,
	 'scheduler/dequeued/memory': 485,
	 'scheduler/enqueued': 485,
	 'scheduler/enqueued/memory': 485,
	 'spider_exceptions/IOError': 3,
	 'spider_exceptions/UnicodeEncodeError': 5,
	 'start_time': datetime.datetime(2015, 7, 28, 21, 37, 20, 440401)}
2015-07-28 18:49:31-0300 [stf] INFO: Spider closed (finished)
2015-07-28 18:49:31-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 18:49:31-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 18:49:31-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 18:49:31-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 18:49:32-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 18:49:32-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 18:49:32-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 18:49:32-0300 [stf] INFO: Spider opened
2015-07-28 18:49:32-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 18:49:41-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=1&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 66, in parseDoc
	    dataJulg    = parser.parseDataJulgamento( ''.join( docHeader[1:]))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFParser.py", line 38, in parseDataJulgamento
	    return datetime( int(date.group(3)), int(date.group(2)), int(date.group(1)))
	exceptions.ValueError: month must be in 1..12
	
2015-07-28 18:49:42-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=67&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 18:49:55-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=562&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc7' in position 5: ordinal not in range(128)
	
2015-07-28 18:50:32-0300 [stf] INFO: Crawled 40 pages (at 40 pages/min), scraped 367 items (at 367 items/min)
2015-07-28 18:51:32-0300 [stf] INFO: Crawled 72 pages (at 32 pages/min), scraped 687 items (at 320 items/min)
2015-07-28 18:51:47-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=499&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc3' in position 18: ordinal not in range(128)
	
2015-07-28 18:52:32-0300 [stf] INFO: Crawled 109 pages (at 37 pages/min), scraped 1048 items (at 361 items/min)
2015-07-28 18:52:57-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=454&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 18:53:32-0300 [stf] INFO: Crawled 150 pages (at 41 pages/min), scraped 1450 items (at 402 items/min)
2015-07-28 18:54:32-0300 [stf] INFO: Crawled 188 pages (at 38 pages/min), scraped 1830 items (at 380 items/min)
2015-07-28 18:55:10-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=365&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 18:55:32-0300 [stf] INFO: Crawled 231 pages (at 43 pages/min), scraped 2251 items (at 421 items/min)
2015-07-28 18:56:32-0300 [stf] INFO: Crawled 273 pages (at 42 pages/min), scraped 2671 items (at 420 items/min)
2015-07-28 18:57:32-0300 [stf] INFO: Crawled 318 pages (at 45 pages/min), scraped 3121 items (at 450 items/min)
2015-07-28 18:58:32-0300 [stf] INFO: Crawled 359 pages (at 41 pages/min), scraped 3531 items (at 410 items/min)
2015-07-28 18:59:32-0300 [stf] INFO: Crawled 402 pages (at 43 pages/min), scraped 3961 items (at 430 items/min)
2015-07-28 19:00:27-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=137&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:00:32-0300 [stf] INFO: Crawled 447 pages (at 45 pages/min), scraped 4409 items (at 448 items/min)
2015-07-28 19:01:16-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=96&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:01:32-0300 [stf] INFO: Crawled 495 pages (at 48 pages/min), scraped 4885 items (at 476 items/min)
2015-07-28 19:02:32-0300 [stf] INFO: Crawled 539 pages (at 44 pages/min), scraped 5325 items (at 440 items/min)
2015-07-28 19:02:49-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=18&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc1' in position 17: ordinal not in range(128)
	
2015-07-28 19:03:07-0300 [stf] INFO: Closing spider (finished)
2015-07-28 19:03:07-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 368350,
	 'downloader/request_count': 563,
	 'downloader/request_method_count/GET': 563,
	 'downloader/response_bytes': 45637980,
	 'downloader/response_count': 563,
	 'downloader/response_status_count/200': 563,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 22, 3, 7, 212375),
	 'item_scraped_count': 5558,
	 'log_count/ERROR': 9,
	 'log_count/INFO': 20,
	 'request_depth_max': 1,
	 'response_received_count': 563,
	 'scheduler/dequeued': 563,
	 'scheduler/dequeued/memory': 563,
	 'scheduler/enqueued': 563,
	 'scheduler/enqueued/memory': 563,
	 'spider_exceptions/IOError': 2,
	 'spider_exceptions/UnicodeEncodeError': 6,
	 'spider_exceptions/ValueError': 1,
	 'start_time': datetime.datetime(2015, 7, 28, 21, 49, 32, 899621)}
2015-07-28 19:03:07-0300 [stf] INFO: Spider closed (finished)
2015-07-28 19:03:07-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 19:03:07-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 19:03:07-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 19:03:07-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 19:03:08-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 19:03:08-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 19:03:08-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 19:03:08-0300 [stf] INFO: Spider opened
2015-07-28 19:03:08-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 19:03:32-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=611&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:03:33-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=610&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:04:08-0300 [stf] INFO: Crawled 41 pages (at 41 pages/min), scraped 382 items (at 382 items/min)
2015-07-28 19:05:08-0300 [stf] INFO: Crawled 78 pages (at 37 pages/min), scraped 752 items (at 370 items/min)
2015-07-28 19:06:08-0300 [stf] INFO: Crawled 114 pages (at 36 pages/min), scraped 1112 items (at 360 items/min)
2015-07-28 19:06:47-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=488&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:06:50-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=485&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 19:07:08-0300 [stf] INFO: Crawled 156 pages (at 42 pages/min), scraped 1526 items (at 414 items/min)
2015-07-28 19:07:19-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=468&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:07:41-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=451&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xc1' in position 10: ordinal not in range(128)
	
2015-07-28 19:07:54-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=442&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:08:08-0300 [stf] INFO: Crawled 197 pages (at 41 pages/min), scraped 1923 items (at 397 items/min)
2015-07-28 19:08:40-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=412&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-28 19:09:08-0300 [stf] INFO: Crawled 240 pages (at 43 pages/min), scraped 2350 items (at 427 items/min)
2015-07-28 19:09:49-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=361&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:10:08-0300 [stf] INFO: Crawled 283 pages (at 43 pages/min), scraped 2772 items (at 422 items/min)
2015-07-28 19:10:43-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=319&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 45, in parsePartes
	    print l
	exceptions.UnicodeEncodeError: 'ascii' codec can't encode character u'\xd3' in position 2: ordinal not in range(128)
	
2015-07-28 19:11:08-0300 [stf] INFO: Crawled 329 pages (at 46 pages/min), scraped 3228 items (at 456 items/min)
2015-07-28 19:12:08-0300 [stf] INFO: Crawled 368 pages (at 39 pages/min), scraped 3618 items (at 390 items/min)
2015-07-28 19:13:08-0300 [stf] INFO: Crawled 408 pages (at 40 pages/min), scraped 4018 items (at 400 items/min)
2015-07-28 19:14:08-0300 [stf] INFO: Crawled 449 pages (at 41 pages/min), scraped 4428 items (at 410 items/min)
2015-07-28 19:15:08-0300 [stf] INFO: Crawled 489 pages (at 40 pages/min), scraped 4828 items (at 400 items/min)
2015-07-28 19:16:08-0300 [stf] INFO: Crawled 530 pages (at 41 pages/min), scraped 5238 items (at 410 items/min)
2015-07-28 19:17:08-0300 [stf] INFO: Crawled 571 pages (at 41 pages/min), scraped 5648 items (at 410 items/min)
2015-07-28 19:18:00-0300 [stf] INFO: Closing spider (finished)
2015-07-28 19:18:00-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 401100,
	 'downloader/request_count': 613,
	 'downloader/request_method_count/GET': 613,
	 'downloader/response_bytes': 50571771,
	 'downloader/response_count': 613,
	 'downloader/response_status_count/200': 613,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 28, 22, 18, 0, 169511),
	 'item_scraped_count': 6068,
	 'log_count/ERROR': 10,
	 'log_count/INFO': 21,
	 'request_depth_max': 1,
	 'response_received_count': 613,
	 'scheduler/dequeued': 613,
	 'scheduler/dequeued/memory': 613,
	 'scheduler/enqueued': 613,
	 'scheduler/enqueued/memory': 613,
	 'spider_exceptions/IOError': 1,
	 'spider_exceptions/IndexError': 1,
	 'spider_exceptions/UnicodeEncodeError': 8,
	 'start_time': datetime.datetime(2015, 7, 28, 22, 3, 8, 559066)}
2015-07-28 19:18:00-0300 [stf] INFO: Spider closed (finished)
2015-07-28 23:43:27-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 23:43:27-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 23:43:27-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 23:43:27-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 23:43:28-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 23:43:28-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 23:43:28-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 23:43:28-0300 [stf] INFO: Spider opened
2015-07-28 23:43:28-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 23:44:28-0300 [stf] INFO: Crawled 32 pages (at 32 pages/min), scraped 308 items (at 308 items/min)
2015-07-28 23:45:28-0300 [stf] INFO: Crawled 71 pages (at 39 pages/min), scraped 698 items (at 390 items/min)
2015-07-28 23:46:28-0300 [stf] INFO: Crawled 108 pages (at 37 pages/min), scraped 1068 items (at 370 items/min)
2015-07-28 23:47:28-0300 [stf] INFO: Crawled 146 pages (at 38 pages/min), scraped 1448 items (at 380 items/min)
2015-07-28 23:48:28-0300 [stf] INFO: Crawled 189 pages (at 43 pages/min), scraped 1878 items (at 430 items/min)
2015-07-28 23:48:53-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20010101%29%28%40JULG+%3C%3D+20020101%29&pagina=116&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-28 23:49:28-0300 [stf] INFO: Crawled 221 pages (at 32 pages/min), scraped 2197 items (at 319 items/min)
2015-07-28 23:50:28-0300 [stf] INFO: Crawled 255 pages (at 34 pages/min), scraped 2537 items (at 340 items/min)
2015-07-28 23:51:28-0300 [stf] INFO: Crawled 289 pages (at 34 pages/min), scraped 2877 items (at 340 items/min)
2015-07-28 23:51:47-0300 [stf] INFO: Closing spider (finished)
2015-07-28 23:51:47-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 196740,
	 'downloader/request_count': 301,
	 'downloader/request_method_count/GET': 301,
	 'downloader/response_bytes': 23838170,
	 'downloader/response_count': 301,
	 'downloader/response_status_count/200': 301,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 2, 51, 47, 677032),
	 'item_scraped_count': 2997,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 15,
	 'request_depth_max': 1,
	 'response_received_count': 301,
	 'scheduler/dequeued': 301,
	 'scheduler/dequeued/memory': 301,
	 'scheduler/enqueued': 301,
	 'scheduler/enqueued/memory': 301,
	 'spider_exceptions/IOError': 1,
	 'start_time': datetime.datetime(2015, 7, 29, 2, 43, 28, 346236)}
2015-07-28 23:51:47-0300 [stf] INFO: Spider closed (finished)
2015-07-28 23:51:47-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-28 23:51:47-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-28 23:51:47-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-28 23:51:47-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-28 23:51:48-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-28 23:51:48-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-28 23:51:49-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-28 23:51:49-0300 [stf] INFO: Spider opened
2015-07-28 23:51:49-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-28 23:52:49-0300 [stf] INFO: Crawled 34 pages (at 34 pages/min), scraped 323 items (at 323 items/min)
2015-07-28 23:53:49-0300 [stf] INFO: Crawled 71 pages (at 37 pages/min), scraped 693 items (at 370 items/min)
2015-07-28 23:54:49-0300 [stf] INFO: Crawled 107 pages (at 36 pages/min), scraped 1053 items (at 360 items/min)
2015-07-28 23:55:49-0300 [stf] INFO: Crawled 142 pages (at 35 pages/min), scraped 1403 items (at 350 items/min)
2015-07-28 23:56:49-0300 [stf] INFO: Crawled 179 pages (at 37 pages/min), scraped 1764 items (at 361 items/min)
2015-07-28 23:57:49-0300 [stf] INFO: Crawled 215 pages (at 36 pages/min), scraped 2133 items (at 369 items/min)
2015-07-28 23:58:49-0300 [stf] INFO: Crawled 255 pages (at 40 pages/min), scraped 2533 items (at 400 items/min)
2015-07-28 23:59:49-0300 [stf] INFO: Crawled 292 pages (at 37 pages/min), scraped 2903 items (at 370 items/min)
2015-07-29 00:00:49-0300 [stf] INFO: Crawled 327 pages (at 35 pages/min), scraped 3253 items (at 350 items/min)
2015-07-29 00:01:49-0300 [stf] INFO: Crawled 365 pages (at 38 pages/min), scraped 3633 items (at 380 items/min)
2015-07-29 00:02:19-0300 [stf] INFO: Closing spider (finished)
2015-07-29 00:02:19-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 251105,
	 'downloader/request_count': 384,
	 'downloader/request_method_count/GET': 384,
	 'downloader/response_bytes': 30436728,
	 'downloader/response_count': 384,
	 'downloader/response_status_count/200': 384,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 3, 2, 19, 686309),
	 'item_scraped_count': 3823,
	 'log_count/INFO': 17,
	 'request_depth_max': 1,
	 'response_received_count': 384,
	 'scheduler/dequeued': 384,
	 'scheduler/dequeued/memory': 384,
	 'scheduler/enqueued': 384,
	 'scheduler/enqueued/memory': 384,
	 'start_time': datetime.datetime(2015, 7, 29, 2, 51, 49, 14630)}
2015-07-29 00:02:19-0300 [stf] INFO: Spider closed (finished)
2015-07-29 00:02:19-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 00:02:19-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 00:02:19-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 00:02:19-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 00:02:21-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 00:02:21-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 00:02:21-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 00:02:21-0300 [stf] INFO: Spider opened
2015-07-29 00:02:21-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 00:03:21-0300 [stf] INFO: Crawled 35 pages (at 35 pages/min), scraped 339 items (at 339 items/min)
2015-07-29 00:04:21-0300 [stf] INFO: Crawled 69 pages (at 34 pages/min), scraped 679 items (at 340 items/min)
2015-07-29 00:05:21-0300 [stf] INFO: Crawled 102 pages (at 33 pages/min), scraped 1009 items (at 330 items/min)
2015-07-29 00:06:21-0300 [stf] INFO: Crawled 133 pages (at 31 pages/min), scraped 1319 items (at 310 items/min)
2015-07-29 00:07:21-0300 [stf] INFO: Crawled 164 pages (at 31 pages/min), scraped 1629 items (at 310 items/min)
2015-07-29 00:08:21-0300 [stf] INFO: Crawled 197 pages (at 33 pages/min), scraped 1959 items (at 330 items/min)
2015-07-29 00:08:23-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20030101%29%28%40JULG+%3C%3D+20040101%29&pagina=53&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:09:21-0300 [stf] INFO: Crawled 230 pages (at 33 pages/min), scraped 2288 items (at 329 items/min)
2015-07-29 00:09:29-0300 [stf] INFO: Closing spider (finished)
2015-07-29 00:09:29-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 153510,
	 'downloader/request_count': 235,
	 'downloader/request_method_count/GET': 235,
	 'downloader/response_bytes': 18953387,
	 'downloader/response_count': 235,
	 'downloader/response_status_count/200': 235,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 3, 9, 29, 678482),
	 'item_scraped_count': 2338,
	 'log_count/ERROR': 1,
	 'log_count/INFO': 14,
	 'request_depth_max': 1,
	 'response_received_count': 235,
	 'scheduler/dequeued': 235,
	 'scheduler/dequeued/memory': 235,
	 'scheduler/enqueued': 235,
	 'scheduler/enqueued/memory': 235,
	 'spider_exceptions/IndexError': 1,
	 'start_time': datetime.datetime(2015, 7, 29, 3, 2, 21, 34597)}
2015-07-29 00:09:29-0300 [stf] INFO: Spider closed (finished)
2015-07-29 00:09:29-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 00:09:29-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 00:09:29-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 00:09:29-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 00:09:30-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 00:09:30-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 00:09:31-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 00:09:31-0300 [stf] INFO: Spider opened
2015-07-29 00:09:31-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 00:10:31-0300 [stf] INFO: Crawled 33 pages (at 33 pages/min), scraped 316 items (at 316 items/min)
2015-07-29 00:11:31-0300 [stf] INFO: Crawled 69 pages (at 36 pages/min), scraped 676 items (at 360 items/min)
2015-07-29 00:12:31-0300 [stf] INFO: Crawled 98 pages (at 29 pages/min), scraped 966 items (at 290 items/min)
2015-07-29 00:13:27-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=245&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:13:31-0300 [stf] INFO: Crawled 132 pages (at 34 pages/min), scraped 1298 items (at 332 items/min)
2015-07-29 00:13:57-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=228&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:14:19-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=214&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:14:31-0300 [stf] INFO: Crawled 169 pages (at 37 pages/min), scraped 1663 items (at 365 items/min)
2015-07-29 00:14:38-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=202&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:15:31-0300 [stf] INFO: Crawled 203 pages (at 34 pages/min), scraped 1993 items (at 330 items/min)
2015-07-29 00:16:31-0300 [stf] INFO: Crawled 234 pages (at 31 pages/min), scraped 2303 items (at 310 items/min)
2015-07-29 00:17:26-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=108&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:17:31-0300 [stf] INFO: Crawled 270 pages (at 36 pages/min), scraped 2653 items (at 350 items/min)
2015-07-29 00:18:31-0300 [stf] INFO: Crawled 306 pages (at 36 pages/min), scraped 3013 items (at 360 items/min)
2015-07-29 00:19:31-0300 [stf] INFO: Crawled 343 pages (at 37 pages/min), scraped 3383 items (at 370 items/min)
2015-07-29 00:19:41-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20040101%29%28%40JULG+%3C%3D+20050101%29&pagina=13&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:19:57-0300 [stf] INFO: Closing spider (finished)
2015-07-29 00:19:57-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 234730,
	 'downloader/request_count': 359,
	 'downloader/request_method_count/GET': 359,
	 'downloader/response_bytes': 28581626,
	 'downloader/response_count': 359,
	 'downloader/response_status_count/200': 359,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 3, 19, 57, 170743),
	 'item_scraped_count': 3537,
	 'log_count/ERROR': 6,
	 'log_count/INFO': 17,
	 'request_depth_max': 1,
	 'response_received_count': 359,
	 'scheduler/dequeued': 359,
	 'scheduler/dequeued/memory': 359,
	 'scheduler/enqueued': 359,
	 'scheduler/enqueued/memory': 359,
	 'spider_exceptions/IndexError': 6,
	 'start_time': datetime.datetime(2015, 7, 29, 3, 9, 31, 20435)}
2015-07-29 00:19:57-0300 [stf] INFO: Spider closed (finished)
2015-07-29 00:19:57-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 00:19:57-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 00:19:57-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 00:19:57-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 00:19:58-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 00:19:58-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 00:19:58-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 00:19:58-0300 [stf] INFO: Spider opened
2015-07-29 00:19:58-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 00:20:58-0300 [stf] INFO: Crawled 37 pages (at 37 pages/min), scraped 358 items (at 358 items/min)
2015-07-29 00:21:58-0300 [stf] INFO: Crawled 76 pages (at 39 pages/min), scraped 748 items (at 390 items/min)
2015-07-29 00:22:58-0300 [stf] INFO: Crawled 118 pages (at 42 pages/min), scraped 1168 items (at 420 items/min)
2015-07-29 00:23:40-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=318&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:23:58-0300 [stf] INFO: Crawled 165 pages (at 47 pages/min), scraped 1636 items (at 468 items/min)
2015-07-29 00:24:58-0300 [stf] INFO: Crawled 212 pages (at 47 pages/min), scraped 2106 items (at 470 items/min)
2015-07-29 00:25:58-0300 [stf] INFO: Crawled 249 pages (at 37 pages/min), scraped 2476 items (at 370 items/min)
2015-07-29 00:26:48-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=186&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:26:58-0300 [stf] INFO: Crawled 285 pages (at 36 pages/min), scraped 2832 items (at 356 items/min)
2015-07-29 00:27:21-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20050101%29%28%40JULG+%3C%3D+20060101%29&pagina=171&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:27:58-0300 [stf] INFO: Crawled 317 pages (at 32 pages/min), scraped 3148 items (at 316 items/min)
2015-07-29 00:28:58-0300 [stf] INFO: Crawled 355 pages (at 38 pages/min), scraped 3528 items (at 380 items/min)
2015-07-29 00:29:58-0300 [stf] INFO: Crawled 394 pages (at 39 pages/min), scraped 3918 items (at 390 items/min)
2015-07-29 00:30:58-0300 [stf] INFO: Crawled 430 pages (at 36 pages/min), scraped 4278 items (at 360 items/min)
2015-07-29 00:31:37-0300 [stf] INFO: Closing spider (finished)
2015-07-29 00:31:37-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 295645,
	 'downloader/request_count': 452,
	 'downloader/request_method_count/GET': 452,
	 'downloader/response_bytes': 36570022,
	 'downloader/response_count': 452,
	 'downloader/response_status_count/200': 452,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 3, 31, 37, 148344),
	 'item_scraped_count': 4498,
	 'log_count/ERROR': 3,
	 'log_count/INFO': 18,
	 'request_depth_max': 1,
	 'response_received_count': 452,
	 'scheduler/dequeued': 452,
	 'scheduler/dequeued/memory': 452,
	 'scheduler/enqueued': 452,
	 'scheduler/enqueued/memory': 452,
	 'spider_exceptions/IndexError': 3,
	 'start_time': datetime.datetime(2015, 7, 29, 3, 19, 58, 509187)}
2015-07-29 00:31:37-0300 [stf] INFO: Spider closed (finished)
2015-07-29 00:31:37-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 00:31:37-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 00:31:37-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 00:31:37-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 00:31:38-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 00:31:38-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 00:31:38-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 00:31:38-0300 [stf] INFO: Spider opened
2015-07-29 00:31:38-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 00:32:38-0300 [stf] INFO: Crawled 38 pages (at 38 pages/min), scraped 361 items (at 361 items/min)
2015-07-29 00:33:38-0300 [stf] INFO: Crawled 69 pages (at 31 pages/min), scraped 671 items (at 310 items/min)
2015-07-29 00:33:57-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=342&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:34:38-0300 [stf] INFO: Crawled 95 pages (at 26 pages/min), scraped 921 items (at 250 items/min)
2015-07-29 00:35:38-0300 [stf] INFO: Crawled 127 pages (at 32 pages/min), scraped 1241 items (at 320 items/min)
2015-07-29 00:36:38-0300 [stf] INFO: Crawled 163 pages (at 36 pages/min), scraped 1601 items (at 360 items/min)
2015-07-29 00:36:43-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=251&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:37:38-0300 [stf] INFO: Crawled 204 pages (at 41 pages/min), scraped 2007 items (at 406 items/min)
2015-07-29 00:38:33-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=178&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:38:38-0300 [stf] INFO: Crawled 244 pages (at 40 pages/min), scraped 2399 items (at 392 items/min)
2015-07-29 00:39:38-0300 [stf] INFO: Crawled 285 pages (at 41 pages/min), scraped 2809 items (at 410 items/min)
2015-07-29 00:40:26-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=104&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:40:38-0300 [stf] INFO: Crawled 323 pages (at 38 pages/min), scraped 3179 items (at 370 items/min)
2015-07-29 00:40:47-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20060101%29%28%40JULG+%3C%3D+20070101%29&pagina=89&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:41:38-0300 [stf] INFO: Crawled 364 pages (at 41 pages/min), scraped 3581 items (at 402 items/min)
2015-07-29 00:42:38-0300 [stf] INFO: Crawled 400 pages (at 36 pages/min), scraped 3941 items (at 360 items/min)
2015-07-29 00:42:40-0300 [stf] INFO: Closing spider (finished)
2015-07-29 00:42:40-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 262895,
	 'downloader/request_count': 402,
	 'downloader/request_method_count/GET': 402,
	 'downloader/response_bytes': 32355958,
	 'downloader/response_count': 402,
	 'downloader/response_status_count/200': 402,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 3, 42, 40, 967951),
	 'item_scraped_count': 3961,
	 'log_count/ERROR': 5,
	 'log_count/INFO': 18,
	 'request_depth_max': 1,
	 'response_received_count': 402,
	 'scheduler/dequeued': 402,
	 'scheduler/dequeued/memory': 402,
	 'scheduler/enqueued': 402,
	 'scheduler/enqueued/memory': 402,
	 'spider_exceptions/IndexError': 5,
	 'start_time': datetime.datetime(2015, 7, 29, 3, 31, 38, 502327)}
2015-07-29 00:42:40-0300 [stf] INFO: Spider closed (finished)
2015-07-29 00:42:41-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 00:42:41-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 00:42:41-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 00:42:41-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 00:42:42-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 00:42:42-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 00:42:42-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 00:42:42-0300 [stf] INFO: Spider opened
2015-07-29 00:42:42-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 00:43:42-0300 [stf] INFO: Crawled 40 pages (at 40 pages/min), scraped 388 items (at 388 items/min)
2015-07-29 00:44:05-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=420&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:44:42-0300 [stf] INFO: Crawled 78 pages (at 38 pages/min), scraped 762 items (at 374 items/min)
2015-07-29 00:45:42-0300 [stf] INFO: Crawled 120 pages (at 42 pages/min), scraped 1182 items (at 420 items/min)
2015-07-29 00:46:42-0300 [stf] INFO: Crawled 158 pages (at 38 pages/min), scraped 1562 items (at 380 items/min)
2015-07-29 00:47:42-0300 [stf] INFO: Crawled 198 pages (at 40 pages/min), scraped 1962 items (at 400 items/min)
2015-07-29 00:48:42-0300 [stf] INFO: Crawled 231 pages (at 33 pages/min), scraped 2292 items (at 330 items/min)
2015-07-29 00:49:42-0300 [stf] INFO: Crawled 269 pages (at 38 pages/min), scraped 2672 items (at 380 items/min)
2015-07-29 00:50:16-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=185&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:50:42-0300 [stf] INFO: Crawled 310 pages (at 41 pages/min), scraped 3078 items (at 406 items/min)
2015-07-29 00:51:42-0300 [stf] INFO: Crawled 349 pages (at 39 pages/min), scraped 3468 items (at 390 items/min)
2015-07-29 00:52:29-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20070101%29%28%40JULG+%3C%3D+20080101%29&pagina=102&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 00:52:42-0300 [stf] INFO: Crawled 383 pages (at 34 pages/min), scraped 3804 items (at 336 items/min)
2015-07-29 00:53:42-0300 [stf] INFO: Crawled 422 pages (at 39 pages/min), scraped 4194 items (at 390 items/min)
2015-07-29 00:54:36-0300 [stf] INFO: Closing spider (finished)
2015-07-29 00:54:36-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 300885,
	 'downloader/request_count': 460,
	 'downloader/request_method_count/GET': 460,
	 'downloader/response_bytes': 39778969,
	 'downloader/response_count': 460,
	 'downloader/response_status_count/200': 460,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 3, 54, 36, 54295),
	 'item_scraped_count': 4574,
	 'log_count/ERROR': 3,
	 'log_count/INFO': 18,
	 'request_depth_max': 1,
	 'response_received_count': 460,
	 'scheduler/dequeued': 460,
	 'scheduler/dequeued/memory': 460,
	 'scheduler/enqueued': 460,
	 'scheduler/enqueued/memory': 460,
	 'spider_exceptions/IndexError': 3,
	 'start_time': datetime.datetime(2015, 7, 29, 3, 42, 42, 317364)}
2015-07-29 00:54:36-0300 [stf] INFO: Spider closed (finished)
2015-07-29 00:54:36-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 00:54:36-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 00:54:36-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 00:54:36-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 00:54:37-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 00:54:37-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 00:54:37-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 00:54:37-0300 [stf] INFO: Spider opened
2015-07-29 00:54:37-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 00:55:37-0300 [stf] INFO: Crawled 34 pages (at 34 pages/min), scraped 327 items (at 327 items/min)
2015-07-29 00:56:37-0300 [stf] INFO: Crawled 68 pages (at 34 pages/min), scraped 667 items (at 340 items/min)
2015-07-29 00:57:37-0300 [stf] INFO: Crawled 103 pages (at 35 pages/min), scraped 1017 items (at 350 items/min)
2015-07-29 00:58:37-0300 [stf] INFO: Crawled 143 pages (at 40 pages/min), scraped 1417 items (at 400 items/min)
2015-07-29 00:59:37-0300 [stf] INFO: Crawled 183 pages (at 40 pages/min), scraped 1817 items (at 400 items/min)
2015-07-29 01:00:00-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=316&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:00:37-0300 [stf] INFO: Crawled 225 pages (at 42 pages/min), scraped 2231 items (at 414 items/min)
2015-07-29 01:01:37-0300 [stf] INFO: Crawled 269 pages (at 44 pages/min), scraped 2662 items (at 431 items/min)
2015-07-29 01:01:44-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=240&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:02:37-0300 [stf] INFO: Crawled 311 pages (at 42 pages/min), scraped 3085 items (at 423 items/min)
2015-07-29 01:03:37-0300 [stf] INFO: Crawled 351 pages (at 40 pages/min), scraped 3485 items (at 400 items/min)
2015-07-29 01:04:37-0300 [stf] INFO: Crawled 389 pages (at 38 pages/min), scraped 3865 items (at 380 items/min)
2015-07-29 01:04:54-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=113&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:05:37-0300 [stf] INFO: Crawled 433 pages (at 44 pages/min), scraped 4304 items (at 439 items/min)
2015-07-29 01:06:37-0300 [stf] INFO: Crawled 474 pages (at 41 pages/min), scraped 4714 items (at 410 items/min)
2015-07-29 01:07:21-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20080101%29%28%40JULG+%3C%3D+20090101%29&pagina=2&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:07:21-0300 [stf] INFO: Closing spider (finished)
2015-07-29 01:07:21-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 325775,
	 'downloader/request_count': 498,
	 'downloader/request_method_count/GET': 498,
	 'downloader/response_bytes': 41298494,
	 'downloader/response_count': 498,
	 'downloader/response_status_count/200': 498,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 4, 7, 21, 849599),
	 'item_scraped_count': 4947,
	 'log_count/ERROR': 4,
	 'log_count/INFO': 19,
	 'request_depth_max': 1,
	 'response_received_count': 498,
	 'scheduler/dequeued': 498,
	 'scheduler/dequeued/memory': 498,
	 'scheduler/enqueued': 498,
	 'scheduler/enqueued/memory': 498,
	 'spider_exceptions/IndexError': 4,
	 'start_time': datetime.datetime(2015, 7, 29, 3, 54, 37, 403382)}
2015-07-29 01:07:21-0300 [stf] INFO: Spider closed (finished)
2015-07-29 01:07:22-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 01:07:22-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 01:07:22-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 01:07:22-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 01:07:23-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 01:07:23-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 01:07:23-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 01:07:23-0300 [stf] INFO: Spider opened
2015-07-29 01:07:23-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 01:08:23-0300 [stf] INFO: Crawled 35 pages (at 35 pages/min), scraped 340 items (at 340 items/min)
2015-07-29 01:09:23-0300 [stf] INFO: Crawled 74 pages (at 39 pages/min), scraped 730 items (at 390 items/min)
2015-07-29 01:10:23-0300 [stf] INFO: Crawled 114 pages (at 40 pages/min), scraped 1130 items (at 400 items/min)
2015-07-29 01:11:23-0300 [stf] INFO: Crawled 145 pages (at 31 pages/min), scraped 1440 items (at 310 items/min)
2015-07-29 01:12:23-0300 [stf] INFO: Crawled 181 pages (at 36 pages/min), scraped 1800 items (at 360 items/min)
2015-07-29 01:13:23-0300 [stf] INFO: Crawled 216 pages (at 35 pages/min), scraped 2150 items (at 350 items/min)
2015-07-29 01:14:23-0300 [stf] INFO: Crawled 250 pages (at 34 pages/min), scraped 2490 items (at 340 items/min)
2015-07-29 01:15:14-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=191&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:15:23-0300 [stf] INFO: Crawled 284 pages (at 34 pages/min), scraped 2822 items (at 332 items/min)
2015-07-29 01:15:24-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=185&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:16:21-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20090101%29%28%40JULG+%3C%3D+20100101%29&pagina=156&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:16:23-0300 [stf] INFO: Crawled 315 pages (at 31 pages/min), scraped 3120 items (at 298 items/min)
2015-07-29 01:17:23-0300 [stf] INFO: Crawled 348 pages (at 33 pages/min), scraped 3450 items (at 330 items/min)
2015-07-29 01:18:23-0300 [stf] INFO: Crawled 389 pages (at 41 pages/min), scraped 3860 items (at 410 items/min)
2015-07-29 01:19:23-0300 [stf] INFO: Crawled 431 pages (at 42 pages/min), scraped 4280 items (at 420 items/min)
2015-07-29 01:19:59-0300 [stf] INFO: Closing spider (finished)
2015-07-29 01:19:59-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 296300,
	 'downloader/request_count': 453,
	 'downloader/request_method_count/GET': 453,
	 'downloader/response_bytes': 36324639,
	 'downloader/response_count': 453,
	 'downloader/response_status_count/200': 453,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 4, 19, 59, 694183),
	 'item_scraped_count': 4500,
	 'log_count/ERROR': 3,
	 'log_count/INFO': 19,
	 'request_depth_max': 1,
	 'response_received_count': 453,
	 'scheduler/dequeued': 453,
	 'scheduler/dequeued/memory': 453,
	 'scheduler/enqueued': 453,
	 'scheduler/enqueued/memory': 453,
	 'spider_exceptions/IndexError': 3,
	 'start_time': datetime.datetime(2015, 7, 29, 4, 7, 23, 188469)}
2015-07-29 01:19:59-0300 [stf] INFO: Spider closed (finished)
2015-07-29 01:19:59-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 01:19:59-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 01:19:59-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 01:19:59-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 01:20:01-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 01:20:01-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 01:20:01-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 01:20:01-0300 [stf] INFO: Spider opened
2015-07-29 01:20:01-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 01:21:01-0300 [stf] INFO: Crawled 42 pages (at 42 pages/min), scraped 404 items (at 404 items/min)
2015-07-29 01:22:01-0300 [stf] INFO: Crawled 84 pages (at 42 pages/min), scraped 824 items (at 420 items/min)
2015-07-29 01:23:01-0300 [stf] INFO: Crawled 123 pages (at 39 pages/min), scraped 1214 items (at 390 items/min)
2015-07-29 01:24:01-0300 [stf] INFO: Crawled 169 pages (at 46 pages/min), scraped 1674 items (at 460 items/min)
2015-07-29 01:24:09-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=325&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-29 01:24:35-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=302&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-29 01:25:01-0300 [stf] INFO: Crawled 220 pages (at 51 pages/min), scraped 2173 items (at 499 items/min)
2015-07-29 01:26:01-0300 [stf] INFO: Crawled 257 pages (at 37 pages/min), scraped 2543 items (at 370 items/min)
2015-07-29 01:27:01-0300 [stf] INFO: Crawled 300 pages (at 43 pages/min), scraped 2973 items (at 430 items/min)
2015-07-29 01:28:01-0300 [stf] INFO: Crawled 339 pages (at 39 pages/min), scraped 3363 items (at 390 items/min)
2015-07-29 01:29:01-0300 [stf] INFO: Crawled 387 pages (at 48 pages/min), scraped 3843 items (at 480 items/min)
2015-07-29 01:29:50-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20100101%29%28%40JULG+%3C%3D+20110101%29&pagina=84&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-29 01:30:01-0300 [stf] INFO: Crawled 427 pages (at 40 pages/min), scraped 4234 items (at 391 items/min)
2015-07-29 01:31:01-0300 [stf] INFO: Crawled 468 pages (at 41 pages/min), scraped 4644 items (at 410 items/min)
2015-07-29 01:31:27-0300 [stf] INFO: Closing spider (finished)
2015-07-29 01:31:27-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 317260,
	 'downloader/request_count': 485,
	 'downloader/request_method_count/GET': 485,
	 'downloader/response_bytes': 38911978,
	 'downloader/response_count': 485,
	 'downloader/response_status_count/200': 485,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 4, 31, 27, 963682),
	 'item_scraped_count': 4814,
	 'log_count/ERROR': 3,
	 'log_count/INFO': 18,
	 'request_depth_max': 1,
	 'response_received_count': 485,
	 'scheduler/dequeued': 485,
	 'scheduler/dequeued/memory': 485,
	 'scheduler/enqueued': 485,
	 'scheduler/enqueued/memory': 485,
	 'spider_exceptions/IOError': 3,
	 'start_time': datetime.datetime(2015, 7, 29, 4, 20, 1, 36419)}
2015-07-29 01:31:27-0300 [stf] INFO: Spider closed (finished)
2015-07-29 01:31:28-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 01:31:28-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 01:31:28-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 01:31:28-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 01:31:29-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 01:31:29-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 01:31:29-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 01:31:29-0300 [stf] INFO: Spider opened
2015-07-29 01:31:29-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 01:32:29-0300 [stf] INFO: Crawled 39 pages (at 39 pages/min), scraped 374 items (at 374 items/min)
2015-07-29 01:33:29-0300 [stf] INFO: Crawled 70 pages (at 31 pages/min), scraped 684 items (at 310 items/min)
2015-07-29 01:34:29-0300 [stf] INFO: Crawled 107 pages (at 37 pages/min), scraped 1054 items (at 370 items/min)
2015-07-29 01:34:57-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=454&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-29 01:35:29-0300 [stf] INFO: Crawled 147 pages (at 40 pages/min), scraped 1446 items (at 392 items/min)
2015-07-29 01:36:29-0300 [stf] INFO: Crawled 184 pages (at 37 pages/min), scraped 1816 items (at 370 items/min)
2015-07-29 01:37:13-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=365&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-29 01:37:29-0300 [stf] INFO: Crawled 225 pages (at 41 pages/min), scraped 2217 items (at 401 items/min)
2015-07-29 01:38:29-0300 [stf] INFO: Crawled 267 pages (at 42 pages/min), scraped 2637 items (at 420 items/min)
2015-07-29 01:39:29-0300 [stf] INFO: Crawled 309 pages (at 42 pages/min), scraped 3057 items (at 420 items/min)
2015-07-29 01:40:29-0300 [stf] INFO: Crawled 345 pages (at 36 pages/min), scraped 3417 items (at 360 items/min)
2015-07-29 01:41:29-0300 [stf] INFO: Crawled 389 pages (at 44 pages/min), scraped 3857 items (at 440 items/min)
2015-07-29 01:42:29-0300 [stf] INFO: Crawled 431 pages (at 42 pages/min), scraped 4277 items (at 420 items/min)
2015-07-29 01:43:29-0300 [stf] INFO: Crawled 475 pages (at 44 pages/min), scraped 4717 items (at 440 items/min)
2015-07-29 01:44:29-0300 [stf] INFO: Crawled 521 pages (at 46 pages/min), scraped 5177 items (at 460 items/min)
2015-07-29 01:45:29-0300 [stf] INFO: Crawled 560 pages (at 39 pages/min), scraped 5567 items (at 390 items/min)
2015-07-29 01:45:32-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20110101%29%28%40JULG+%3C%3D+20120101%29&pagina=1&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 66, in parseDoc
	    dataJulg    = parser.parseDataJulgamento( ''.join( docHeader[1:]))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFParser.py", line 38, in parseDataJulgamento
	    return datetime( int(date.group(3)), int(date.group(2)), int(date.group(1)))
	exceptions.ValueError: month must be in 1..12
	
2015-07-29 01:45:32-0300 [stf] INFO: Closing spider (finished)
2015-07-29 01:45:32-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 368350,
	 'downloader/request_count': 563,
	 'downloader/request_method_count/GET': 563,
	 'downloader/response_bytes': 45638576,
	 'downloader/response_count': 563,
	 'downloader/response_status_count/200': 563,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 4, 45, 32, 992043),
	 'item_scraped_count': 5587,
	 'log_count/ERROR': 3,
	 'log_count/INFO': 21,
	 'request_depth_max': 1,
	 'response_received_count': 563,
	 'scheduler/dequeued': 563,
	 'scheduler/dequeued/memory': 563,
	 'scheduler/enqueued': 563,
	 'scheduler/enqueued/memory': 563,
	 'spider_exceptions/IOError': 2,
	 'spider_exceptions/ValueError': 1,
	 'start_time': datetime.datetime(2015, 7, 29, 4, 31, 29, 311181)}
2015-07-29 01:45:32-0300 [stf] INFO: Spider closed (finished)
2015-07-29 01:45:33-0300 [scrapy] INFO: Scrapy 0.22.2 started (bot: acordaos)
2015-07-29 01:45:33-0300 [scrapy] INFO: Optional features available: ssl, http11, boto
2015-07-29 01:45:33-0300 [scrapy] INFO: Overridden settings: {'NEWSPIDER_MODULE': 'acordaos.spiders_dev', 'SPIDER_MODULES': ['acordaos.spiders'], 'LOG_FILE': 'logstf$i', 'LOG_LEVEL': 'INFO', 'BOT_NAME': 'acordaos'}
2015-07-29 01:45:33-0300 [scrapy] INFO: Enabled extensions: LogStats, TelnetConsole, CloseSpider, WebService, CoreStats, SpiderState
2015-07-29 01:45:34-0300 [scrapy] INFO: Enabled downloader middlewares: HttpAuthMiddleware, DownloadTimeoutMiddleware, UserAgentMiddleware, RetryMiddleware, DefaultHeadersMiddleware, MetaRefreshMiddleware, HttpCompressionMiddleware, RedirectMiddleware, CookiesMiddleware, ChunkedTransferMiddleware, DownloaderStats
2015-07-29 01:45:34-0300 [scrapy] INFO: Enabled spider middlewares: HttpErrorMiddleware, OffsiteMiddleware, RefererMiddleware, UrlLengthMiddleware, DepthMiddleware
2015-07-29 01:45:34-0300 [scrapy] INFO: Enabled item pipelines: MongoDBPipeline
2015-07-29 01:45:34-0300 [stf] INFO: Spider opened
2015-07-29 01:45:34-0300 [stf] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)
2015-07-29 01:46:34-0300 [stf] INFO: Crawled 39 pages (at 39 pages/min), scraped 374 items (at 374 items/min)
2015-07-29 01:47:34-0300 [stf] INFO: Crawled 77 pages (at 38 pages/min), scraped 754 items (at 380 items/min)
2015-07-29 01:48:34-0300 [stf] INFO: Crawled 118 pages (at 41 pages/min), scraped 1164 items (at 410 items/min)
2015-07-29 01:49:12-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=485&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 47, in parsePartes
	    print partes
	exceptions.IOError: [Errno 5] Input/output error
	
2015-07-29 01:49:34-0300 [stf] INFO: Crawled 158 pages (at 40 pages/min), scraped 1562 items (at 398 items/min)
2015-07-29 01:50:34-0300 [stf] INFO: Crawled 200 pages (at 42 pages/min), scraped 1982 items (at 420 items/min)
2015-07-29 01:51:01-0300 [stf] ERROR: Spider error processing <GET http://www.stf.jus.br/portal/jurisprudencia/listarJurisprudencia.asp?s1=%28%40JULG+%3E%3D+20120101%29%28%40JULG+%3C%3D+20130101%29&pagina=412&base=baseAcordaos>
	Traceback (most recent call last):
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/base.py", line 824, in runUntilCurrent
	    call.func(*call.args, **call.kw)
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 638, in _tick
	    taskObj._oneWorkUnit()
	  File "/usr/lib/python2.7/dist-packages/twisted/internet/task.py", line 484, in _oneWorkUnit
	    result = next(self._iterator)
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 57, in <genexpr>
	    work = (callable(elem, *args, **named) for elem in iterable)
	--- <exception caught here> ---
	  File "/usr/lib/pymodules/python2.7/scrapy/utils/defer.py", line 96, in iter_errback
	    yield next(it)
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/offsite.py", line 23, in process_spider_output
	    for x in result:
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/referer.py", line 22, in <genexpr>
	    return (_set_referer(r) for r in result or ())
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/urllength.py", line 33, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/usr/lib/pymodules/python2.7/scrapy/contrib/spidermiddleware/depth.py", line 50, in <genexpr>
	    return (r for r in result or () if _filter(r))
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 55, in parsePage
	    yield self.parseDoc( doc)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/STFSpider.py", line 84, in parseDoc
	    partes = parser.parsePartes( partesRaw)
	  File "/home/mayarac/analise-juridica/scrapy/acordaos/spiders/AcordaoParser.py", line 33, in parsePartes
	    partes[-1] = partes[-1]+ " "+l.strip()
	exceptions.IndexError: list index out of range
	
2015-07-29 01:51:34-0300 [stf] INFO: Crawled 243 pages (at 43 pages/min), scraped 2409 items (at 427 items/min)
2015-07-29 01:52:34-0300 [stf] INFO: Crawled 287 pages (at 44 pages/min), scraped 2849 items (at 440 items/min)
2015-07-29 01:53:34-0300 [stf] INFO: Crawled 334 pages (at 47 pages/min), scraped 3319 items (at 470 items/min)
2015-07-29 01:54:34-0300 [stf] INFO: Crawled 371 pages (at 37 pages/min), scraped 3689 items (at 370 items/min)
2015-07-29 01:55:34-0300 [stf] INFO: Crawled 411 pages (at 40 pages/min), scraped 4089 items (at 400 items/min)
2015-07-29 01:56:34-0300 [stf] INFO: Crawled 452 pages (at 41 pages/min), scraped 4499 items (at 410 items/min)
2015-07-29 01:57:34-0300 [stf] INFO: Crawled 494 pages (at 42 pages/min), scraped 4919 items (at 420 items/min)
2015-07-29 01:58:34-0300 [stf] INFO: Crawled 534 pages (at 40 pages/min), scraped 5319 items (at 400 items/min)
2015-07-29 01:59:34-0300 [stf] INFO: Crawled 578 pages (at 44 pages/min), scraped 5759 items (at 440 items/min)
2015-07-29 02:00:17-0300 [stf] INFO: Closing spider (finished)
2015-07-29 02:00:17-0300 [stf] INFO: Dumping Scrapy stats:
	{'downloader/request_bytes': 401100,
	 'downloader/request_count': 613,
	 'downloader/request_method_count/GET': 613,
	 'downloader/response_bytes': 50571771,
	 'downloader/response_count': 613,
	 'downloader/response_status_count/200': 613,
	 'finish_reason': 'finished',
	 'finish_time': datetime.datetime(2015, 7, 29, 5, 0, 17, 94790),
	 'item_scraped_count': 6109,
	 'log_count/ERROR': 2,
	 'log_count/INFO': 21,
	 'request_depth_max': 1,
	 'response_received_count': 613,
	 'scheduler/dequeued': 613,
	 'scheduler/dequeued/memory': 613,
	 'scheduler/enqueued': 613,
	 'scheduler/enqueued/memory': 613,
	 'spider_exceptions/IOError': 1,
	 'spider_exceptions/IndexError': 1,
	 'start_time': datetime.datetime(2015, 7, 29, 4, 45, 34, 349366)}
2015-07-29 02:00:17-0300 [stf] INFO: Spider closed (finished)
